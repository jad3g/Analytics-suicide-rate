{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jad3g/Analytics-suicide-rate/blob/Data-Analysis/Big_Analytics_For_Suicide_Rate_Overview_Rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I-mV0MaWnBY"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm\n",
        "\n"
      ],
      "metadata": {
        "id": "WALEqGotZphP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n"
      ],
      "metadata": {
        "id": "Y2hv9SSXahPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "t54g4t7GglxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh spark-3.0.3-bin-hadoop3.2.tgz\n"
      ],
      "metadata": {
        "id": "BJjmkmpIhl6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf spark-3.0.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "MjqooVhUbP4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "mgDLwpcUcJ5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.3\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "SYHmVofpncoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col, sum\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, KMeansSummary\n",
        "from pyspark.ml.clustering import BisectingKMeans, BisectingKMeansSummary\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "t0ItBhTIpLoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SuicideRatesOverview').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "OkWy7F4srEM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
        "cores"
      ],
      "metadata": {
        "id": "agPFfKagsF4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the Dataset**"
      ],
      "metadata": {
        "id": "QTQwqKzctDQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv('/content/master.csv', inferSchema=True, header=True)\n",
        "data"
      ],
      "metadata": {
        "id": "HvTiOXposYuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kx2hpxbivXje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "id": "RzRDBkWquHRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.toPandas()"
      ],
      "metadata": {
        "id": "AYNLRv9suRvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data validation\n",
        "data.columns"
      ],
      "metadata": {
        "id": "1QbJifbauYI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "mD19XVF7wXAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "IeHu2ogYwb6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.printSchema())\n",
        "print(\"\")\n",
        "print(data.columns)\n",
        "print(\"\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "ZN2KkRBbwylE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DoubleType\n"
      ],
      "metadata": {
        "id": "-UFFiQqbrLpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from struct import Struct\n",
        "from pickle import TRUE\n",
        "data_schema = [StructField(\"country\", StringType(), True),\\\n",
        "               StructField(\"year\", IntegerType(), True),\\\n",
        "               StructField(\"sex\", StringType(), True),\\\n",
        "               StructField(\"age\", StringType(), True),\\\n",
        "               StructField(\"suicide_no\", IntegerType(), True),\\\n",
        "               StructField(\"population\", IntegerType(), True),\\\n",
        "               StructField(\"suicides/100k pop\", DoubleType(), True),\\\n",
        "               StructField(\"country-year\", StringType(), True),\\\n",
        "               StructField(\"HDI for year\", DoubleType(), True),\\\n",
        "               StructField(\"gdp_for_year ($)\", StringType(), True),\\\n",
        "               StructField(\"gdp_per_capita ($)\", IntegerType(), True),\\\n",
        "               StructField(\"generation\", StringType(), True)]"
      ],
      "metadata": {
        "id": "EcF1_JlFrFYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_struc = StructType(fields=data_schema)\n",
        "final_struc"
      ],
      "metadata": {
        "id": "1Wo0RWHrrEpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv('/content/master.csv', schema=final_struc)\n",
        "data"
      ],
      "metadata": {
        "id": "JlUVUUCD00zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "id": "FxdA4mBi1xfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of rows:', data.count())\n",
        "print('Number of columns:', len(data.columns))"
      ],
      "metadata": {
        "id": "glX-qiSU7p3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "5bQlCcweqSAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().show()"
      ],
      "metadata": {
        "id": "VVzdMwO0qkTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the missing values\n",
        "# Check for null values in all columns\n",
        "from pyspark.sql.functions import col, isnan, when, count\n",
        "\n",
        "null_counts = data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns])\n",
        "\n",
        "# Show the counts of null values in each column\n",
        "null_counts.show()"
      ],
      "metadata": {
        "id": "vPuyLbo4y4uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fill = data.fillna(0)\n",
        "data_fill.show()"
      ],
      "metadata": {
        "id": "hQy0CtEO2sZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fill.describe().show()"
      ],
      "metadata": {
        "id": "fdR6-2bo3G8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fill.groupBy(\"country\").count().show()"
      ],
      "metadata": {
        "id": "_a8gfG8K3Wey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fill.groupBy(\"country\").mean(\"suicide_no\").show()"
      ],
      "metadata": {
        "id": "al1JrLGu43Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fill.select(\"suicide_no\", \"suicides/100k pop\", \"HDI for year\", \"gdp_per_capita ($)\").summary(\"count\",\"min\",\"25%\",\"50%\",\"75%\",\"max\").show()"
      ],
      "metadata": {
        "id": "PbnTxPF46Mj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting categorical variables into numerical variables using the String Indexer\n",
        "# create a list of the categorical columns\n",
        "cat_cols = [\"country\", \"sex\", \"age\",\"gdp_for_year ($)\", \"country-year\", \"generation\"]\n",
        "\n",
        "# instantiate string index for the categorical variables\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(data_fill)for col in cat_cols]\n",
        "\n",
        "# apply transformation to dataframe\n",
        "indexed_data = data_fill\n",
        "for indexer in indexers:\n",
        "  indexed_data = indexer.transform(indexed_data)\n",
        "\n",
        "indexed_data.show()\n"
      ],
      "metadata": {
        "id": "o4cfcOTc7Hf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ML = indexed_data.drop(\"country\",\"sex\",\"age\",\"country-year\",\"gdp_for_year ($)\",\"generation\")\n",
        "data_ML.show()"
      ],
      "metadata": {
        "id": "xF3G9GbLSIUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APPLYING PYSPARK MACHINE LEARNING CLUSTERING TECHNIQUE ON SUICIDE RATE OVERVIEW**"
      ],
      "metadata": {
        "id": "LLXMFc27Wwaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a vector assembler for the dataset\n",
        "input_columns = data_ML.columns\n",
        "\n",
        "# create the vector\n",
        "vecAssembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
        "data_ML_KMeans = vecAssembler.transform(data_ML)\n",
        "data_ML_KMeans.show()\n"
      ],
      "metadata": {
        "id": "q4HXyo5pSHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set a max for the number of clusters needed\n",
        "kmax = 50\n",
        "# creating an array filled with zeros for the amount of K\n",
        "kmcost = np.zeros(kmax)\n",
        "for k in range(2,kmax):\n",
        "\n",
        "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
        "    # fit to dataset\n",
        "    model = kmeans.fit(data_ML_KMeans)\n",
        "\n",
        "    # compute the \"cost\" (sum of squared distances) between the input points and their corresponding cluster centers\n",
        "    kmcost[k] = model.summary.trainingCost\n",
        "\n",
        "print(kmcost[2:kmax])\n",
        "\n"
      ],
      "metadata": {
        "id": "zk59ZIDgrjbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the cost vs number of Clusters\n",
        "fig, ax = plt.subplots(1,1, figsize =(10,8))\n",
        "plt.plot(range(2,kmax), kmcost[2:kmax])\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z7aXeHUNtrMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Fit the final model\n",
        "k = 8\n",
        "kmeans = KMeans().setK(k).setSeed(3).setFeaturesCol(\"features\")\n",
        "model = kmeans.fit(data_ML_KMeans)\n",
        "\n",
        "predictions = model.transform(data_ML_KMeans)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette_score = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette Score = \" + str(silhouette_score))"
      ],
      "metadata": {
        "id": "kCc8M5n0tq-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers = model.clusterCenters()\n",
        "for centers in centers:\n",
        "    print(centers)"
      ],
      "metadata": {
        "id": "8s1-u5fMrwz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.toPandas()\n"
      ],
      "metadata": {
        "id": "rvSYY2zwrwts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.groupBy(\"prediction\").agg(min(predictions.suicide_no), max(predictions.suicide_no)).show()"
      ],
      "metadata": {
        "id": "W15VQZb-1P8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BisectingKMeans\n",
        "kmax = 50\n",
        "bkmcost = np.zeros(kmax)\n",
        "for k in range(2, kmax):\n",
        "    bkmeans = BisectingKMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
        "    model_bk = bkmeans.fit(data_ML_KMeans)\n",
        "    bkmcost[k] = model_bk.summary.trainingCost\n",
        "\n",
        "print(bkmcost[2:kmax])\n",
        "\n"
      ],
      "metadata": {
        "id": "aL0Nt0JO1-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1, figsize =(10,8))\n",
        "ax.plot(range(2,kmax),bkmcost[2:kmax])\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k0CnNbkb5ugi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the final model\n",
        "k = 8\n",
        "bkmeans = BisectingKMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
        "model = bkmeans.fit(data_ML_KMeans)\n",
        "\n",
        "predictions = model.transform(data_ML_KMeans)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette_bkmeans_score = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette_bkmeans_score = \" + str(silhouette_bkmeans_score))"
      ],
      "metadata": {
        "id": "4BeWodXc6FUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.groupBy(\"prediction\").agg(min(predictions.suicide_no), max(predictions.suicide_no)).show()"
      ],
      "metadata": {
        "id": "DlO5Aw01-Igu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwMT50qi-l3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}